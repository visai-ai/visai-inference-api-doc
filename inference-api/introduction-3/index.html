<!doctype html>
<html lang="en" dir="ltr" class="plugin-openapi plugin-id-inference-api">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-rc.1">
<title data-rh="true">Introduction | VISAI API Documentation</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://visai-ai.github.io/visai-inference-api-doc/inference-api/introduction-3"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Introduction | VISAI API Documentation"><meta data-rh="true" name="description" content="**Word tokenization** is a process of defining boundaries between words in a sentence. Tokenization is a method of breaking raw texts into smaller units. Each unit is called a “token” which can be a word, subword, or character. In this model, a token refers to a word. Word tokenization is essential to many Natural Language Processing (NLP) pipelines such as text search, keyword extraction, etc. It is also crucial to doing NLP in Thai language which does not have word boundary in a sentence (no spaces between words).

# Base Model - Dictionary based
&gt; **Provider: PyThaiNLP**

We employ PyThaiNLP&#x27;s dictionary-based word tokenization module¹ for this version of the Thai word tokenization Base model. The dictionary that the Base models used is provided beforehand². As a result, this may not be able to tokenize sentences containing out-of-vocabulary tokens (e.g., product names and person names transliterated from foreign languages). We evaluate the word segmentation performance on the test set VISTEC-TP-TH-2021 corpus³, a collection of 49,997 text samples from Twitter, annotated by Thai linguists.

1. Repository: [PyThaiNLP/nlpo3](https://github.com/PyThaiNLP/nlpo3)
2. The dictionary file that we used is available at [PyThaiNLP/pythainlp](https://raw.githubusercontent.com/PyThaiNLP/pythainlp/v3.0.5/pythainlp/corpus/words_th.txt)
3. VISTEC-TP-TH-2021 corpus is available at [OSKut/VISTEC-TP-TH-2021](https://github.com/mrpeerat/OSKut/tree/main/VISTEC-TP-TH-2021)

# Authentication
Word Tokenization requires API key for API request. Go to [API Key Management](https://web-dev.visai.ai/api-key-management) to create and get your API Key.
  - x-api-key
"><meta data-rh="true" property="og:description" content="**Word tokenization** is a process of defining boundaries between words in a sentence. Tokenization is a method of breaking raw texts into smaller units. Each unit is called a “token” which can be a word, subword, or character. In this model, a token refers to a word. Word tokenization is essential to many Natural Language Processing (NLP) pipelines such as text search, keyword extraction, etc. It is also crucial to doing NLP in Thai language which does not have word boundary in a sentence (no spaces between words).

# Base Model - Dictionary based
&gt; **Provider: PyThaiNLP**

We employ PyThaiNLP&#x27;s dictionary-based word tokenization module¹ for this version of the Thai word tokenization Base model. The dictionary that the Base models used is provided beforehand². As a result, this may not be able to tokenize sentences containing out-of-vocabulary tokens (e.g., product names and person names transliterated from foreign languages). We evaluate the word segmentation performance on the test set VISTEC-TP-TH-2021 corpus³, a collection of 49,997 text samples from Twitter, annotated by Thai linguists.

1. Repository: [PyThaiNLP/nlpo3](https://github.com/PyThaiNLP/nlpo3)
2. The dictionary file that we used is available at [PyThaiNLP/pythainlp](https://raw.githubusercontent.com/PyThaiNLP/pythainlp/v3.0.5/pythainlp/corpus/words_th.txt)
3. VISTEC-TP-TH-2021 corpus is available at [OSKut/VISTEC-TP-TH-2021](https://github.com/mrpeerat/OSKut/tree/main/VISTEC-TP-TH-2021)

# Authentication
Word Tokenization requires API key for API request. Go to [API Key Management](https://web-dev.visai.ai/api-key-management) to create and get your API Key.
  - x-api-key
"><link data-rh="true" rel="icon" href="/visai-inference-api-doc/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://visai-ai.github.io/visai-inference-api-doc/inference-api/introduction-3"><link data-rh="true" rel="alternate" href="https://visai-ai.github.io/visai-inference-api-doc/inference-api/introduction-3" hreflang="en"><link data-rh="true" rel="alternate" href="https://visai-ai.github.io/visai-inference-api-doc/inference-api/introduction-3" hreflang="x-default"><link rel="stylesheet" href="/visai-inference-api-doc/assets/css/styles.5c51ff6e.css">
<link rel="preload" href="/visai-inference-api-doc/assets/js/runtime~main.88a51f8c.js" as="script">
<link rel="preload" href="/visai-inference-api-doc/assets/js/main.0e2da748.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/visai-inference-api-doc/"><div class="navbar__logo"><img src="/visai-inference-api-doc/img/visai.png" alt="VISAI" class="themedImage_ToTc themedImage--light_HNdA"><img src="/visai-inference-api-doc/img/visai.png" alt="VISAI" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">API Documentation</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link" docid="api-doc">API</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/visai-inference-api-doc/inference-api/optical-character-recognition-inference-api">Optical Character Recognition</a></li><li><a class="dropdown__link" href="/visai-inference-api-doc/inference-api/sentiment-anlaysis-inference-api">Sentiment Anlaysis</a></li><li><a class="dropdown__link" href="/visai-inference-api-doc/inference-api/speech-to-text-inference-api">Speech to Text</a></li><li><a class="dropdown__link" href="/visai-inference-api-doc/inference-api/word-tokenization-inference-api">Word Tokenization</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0 api-wrapper"><div class="apiPage_KQrU"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><aside class="apiSidebarContainer_Korj"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/visai-inference-api-doc/inference-api/introduction">Optical Character Recognition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/visai-inference-api-doc/inference-api/introduction-1">Sentiment Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/visai-inference-api-doc/inference-api/introduction-2">Speech to Text</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/visai-inference-api-doc/inference-api/introduction-3">Word Tokenization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/visai-inference-api-doc/inference-api/introduction-3">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/visai-inference-api-doc/inference-api/word-tokenization-inference-api">API</a></div></li></ul></li></ul></nav></div></aside><main class="apiMainContainer_L2xG"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col"><div class="apiItemContainer_gBt2"><article><div class="theme-api-markdown markdown"><span class="theme-doc-version-badge badge badge--secondary">Version: 1.0.0</span><h1>Word Tokenization</h1><p><strong>Word tokenization</strong> is a process of defining boundaries between words in a sentence. Tokenization is a method of breaking raw texts into smaller units. Each unit is called a “token” which can be a word, subword, or character. In this model, a token refers to a word. Word tokenization is essential to many Natural Language Processing (NLP) pipelines such as text search, keyword extraction, etc. It is also crucial to doing NLP in Thai language which does not have word boundary in a sentence (no spaces between words).</p><h1>Base Model - Dictionary based</h1><blockquote><p><strong>Provider: PyThaiNLP</strong></p></blockquote><p>We employ PyThaiNLP&#x27;s dictionary-based word tokenization module¹ for this version of the Thai word tokenization Base model. The dictionary that the Base models used is provided beforehand². As a result, this may not be able to tokenize sentences containing out-of-vocabulary tokens (e.g., product names and person names transliterated from foreign languages). We evaluate the word segmentation performance on the test set VISTEC-TP-TH-2021 corpus³, a collection of 49,997 text samples from Twitter, annotated by Thai linguists.</p><ol><li>Repository: <a href="https://github.com/PyThaiNLP/nlpo3" target="_blank" rel="noopener noreferrer">PyThaiNLP/nlpo3</a></li><li>The dictionary file that we used is available at <a href="https://raw.githubusercontent.com/PyThaiNLP/pythainlp/v3.0.5/pythainlp/corpus/words_th.txt" target="_blank" rel="noopener noreferrer">PyThaiNLP/pythainlp</a></li><li>VISTEC-TP-TH-2021 corpus is available at <a href="https://github.com/mrpeerat/OSKut/tree/main/VISTEC-TP-TH-2021" target="_blank" rel="noopener noreferrer">OSKut/VISTEC-TP-TH-2021</a></li></ol><h1>Authentication</h1><p>Word Tokenization requires API key for API request. Go to <a href="https://web-dev.visai.ai/api-key-management" target="_blank" rel="noopener noreferrer">API Key Management</a> to create and get your API Key.</p><ul><li>x-api-key</li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/visai-inference-api-doc/inference-api/speech-to-text-inference-api"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Speech to Text Inference API</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/visai-inference-api-doc/inference-api/word-tokenization-inference-api"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Word Tokenization Inference API</div></a></nav></div></div><div class="col col--3"></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://visai.ai/" rel="noopener noreferrer" class="footerLogoLink_BH7S"><img src="/visai-inference-api-doc/img/visai.png" alt="Deploys by Netlify" class="themedImage_ToTc themedImage--light_HNdA footer__logo" width="160" height="40"><img src="/visai-inference-api-doc/img/visai.png" alt="Deploys by Netlify" class="themedImage_ToTc themedImage--dark_i4oU footer__logo" width="160" height="40"></a></div><div class="footer__copyright">Copyright © 2022 VISAI. All rights reserved.</div></div></div></footer></div>
<script src="/visai-inference-api-doc/assets/js/runtime~main.88a51f8c.js"></script>
<script src="/visai-inference-api-doc/assets/js/main.0e2da748.js"></script>
</body>
</html>