"use strict";(self.webpackChunkvisai_inference_api_doc=self.webpackChunkvisai_inference_api_doc||[]).push([[767],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var i=n(67294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,i,l=function(e,t){if(null==e)return{};var n,i,l={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var s=i.createContext({}),p=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return i.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,l=e.mdxType,a=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=p(n),m=l,y=u["".concat(s,".").concat(m)]||u[m]||c[m]||a;return n?i.createElement(y,r(r({ref:t},d),{},{components:n})):i.createElement(y,r({ref:t},d))}));function m(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var a=n.length,r=new Array(a);r[0]=u;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:l,r[1]=o;for(var p=2;p<a;p++)r[p]=n[p];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},34855:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>s});var i=n(87462),l=(n(67294),n(3905));const a={},r="Speech Segmentation AI Marketplace",o={type:"api",id:"speech-segmentation-ai-marketplace",unversionedId:"speech-segmentation-ai-marketplace",title:"Speech Segmentation AI Marketplace",description:"",slug:"/speech-segmentation-ai-marketplace",frontMatter:{},api:{tags:["AI Marketplace"],description:"AI can detect human speech from other sounds and is widely used in voice-activated apps.",operationId:"https://speechsegmentation.infer.visai.ai/predict",responses:{200:{description:"Return list of time intervals between the human speech or non-speech.",content:{"application/json":{schema:{type:"object",properties:{status:{type:"string",description:"Status of speech segmentaion request"},data:{type:"array",description:"Result data of speech segmentaion",items:{type:"object",properties:{filename:{type:"string",description:"Name of audio file"},interval:{type:"array",description:"Interval time",items:{type:"object",properties:{start:{type:"string",description:"Start time of the boundary"},end:{type:"string",description:"End time of the boundary"},label:{type:"string",description:"Speech or non-speech in the boundary"}}}}}}}}}}}},204:{description:"No content | No result of transcription"},400:{description:"No audio file | Not found audio file *or* Bad requests | Server cannot or will not process the request"},401:{description:"Unauthorized | Incorrect X-API-Key or X-API-Key not have access to this model"},415:{description:"Can't decode [filename] | Unsupported file format"}},requestBody:{content:{"multipart/form-data":{schema:{type:"object",properties:{files:{type:"string",format:"binary",description:"Audio speech file"}},required:["files"]}}}},parameters:[{schema:{type:"string"},in:"header",name:"X-API-Key",description:"Your API key",required:!0}],method:"post",path:"/predict",servers:[{url:"https://speechsegmentation.infer.visai.ai",description:"Default server"}],info:{title:"Speech Segmentation",version:"1.0.0",description:"Speech Segmentation, also known as Voice activity detection (VAD), is the detection of the human speech or non-speech. The speech segmentation is widely used to facilitate in speech processing such as Automatic Speech Recognition (ASR), and Speech Emotion Recognition (SER).\n\n# Base Model - VISAI Speech Segmentaion\n\nWe used a model from NeMo\xb9 and fine-tune to support Thai language. The model was trained with a speech dataset from Thai SER\xb2, and background datasets from MUSAN\xb3 and ChMusic\u2074. The evaluation set was split from the same source as training data and rebalanced to have the same number of segments in each of the classes. If background noise is very loud, the model will fail to detect some words.\n\n# Authentication\nSpeech Segmentation requires API key for API request. Go to [VISAI Console - API Key](https://console.visai.ai/api-key) to create and get your API Key.\n  - X-API-Key\n"},postman:{name:"Speech Segmentation AI Marketplace",description:{content:"AI can detect human speech from other sounds and is widely used in voice-activated apps.",type:"text/plain"},url:{path:["predict"],host:["{{baseUrl}}"],query:[],variable:[]},header:[{description:{content:"(Required) Your API key",type:"text/plain"},key:"X-API-Key",value:"<string>"},{key:"Content-Type",value:"multipart/form-data"}],method:"POST",body:{mode:"formdata",formdata:[{description:{content:"Audio speech file",type:"text/plain"},key:"files",value:"<binary>",type:"text"}]}}},source:"@site/api/openapi-speech-segmentation.yaml",sourceDirName:".",permalink:"/visai-inference-api-doc/inference-api/speech-segmentation-ai-marketplace",previous:{title:"Introduction",permalink:"/visai-inference-api-doc/inference-api/introduction-3"},next:{title:"Introduction",permalink:"/visai-inference-api-doc/inference-api/introduction-4"}},s=[],p={toc:s};function d(e){let{components:t,...n}=e;return(0,l.kt)("wrapper",(0,i.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"speech-segmentation-ai-marketplace"},"Speech Segmentation AI Marketplace"),(0,l.kt)("p",null,"AI can detect human speech from other sounds and is widely used in voice-activated apps."),(0,l.kt)("table",{style:{display:"table",width:"100%"}},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{style:{textAlign:"left"}},"Header Parameters"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"X-API-Key"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("span",{style:{opacity:"0.6"}}," \u2014 "),(0,l.kt)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"}}," REQUIRED"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Your API key")))))),(0,l.kt)("table",{style:{display:"table",width:"100%"}},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{style:{textAlign:"left"}},"Request Body ",(0,l.kt)("div",null)))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"files"),(0,l.kt)("span",{style:{opacity:"0.6"}}," binary"),(0,l.kt)("span",{style:{opacity:"0.6"}}," \u2014 "),(0,l.kt)("strong",{style:{fontSize:"var(--ifm-code-font-size)",color:"var(--openapi-required)"}}," REQUIRED"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Audio speech file")))))),(0,l.kt)("table",{style:{display:"table",width:"100%"}},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{style:{textAlign:"left"}},"Responses"))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("div",{style:{display:"flex"}},(0,l.kt)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"}},(0,l.kt)("code",null,"200")),(0,l.kt)("div",null,(0,l.kt)("p",null,"Return list of time intervals between the human speech or non-speech."))),(0,l.kt)("div",null,(0,l.kt)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"}},(0,l.kt)("thead",null,(0,l.kt)("tr",null,(0,l.kt)("th",{style:{textAlign:"left"}},"Schema ",(0,l.kt)("div",null)))),(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"status"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Status of speech segmentaion request")))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"data"),(0,l.kt)("span",{style:{opacity:"0.6"}}," object[]"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Result data of speech segmentaion")),(0,l.kt)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"}},(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"filename"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Name of audio file")))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"interval"),(0,l.kt)("span",{style:{opacity:"0.6"}}," object[]"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Interval time")),(0,l.kt)("table",{style:{display:"table",width:"100%",marginTop:"var(--ifm-table-cell-padding)",marginBottom:"0px"}},(0,l.kt)("tbody",null,(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"start"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Start time of the boundary")))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"end"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"End time of the boundary")))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("code",null,"label"),(0,l.kt)("span",{style:{opacity:"0.6"}}," string"),(0,l.kt)("div",{style:{marginTop:"var(--ifm-table-cell-padding)"}},(0,l.kt)("p",null,"Speech or non-speech in the boundary"))))))))))))))))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("div",{style:{display:"flex"}},(0,l.kt)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"}},(0,l.kt)("code",null,"204")),(0,l.kt)("div",null,(0,l.kt)("p",null,"No content | No result of transcription"))),(0,l.kt)("div",null))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("div",{style:{display:"flex"}},(0,l.kt)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"}},(0,l.kt)("code",null,"400")),(0,l.kt)("div",null,(0,l.kt)("p",null,"No audio file | Not found audio file ",(0,l.kt)("em",{parentName:"p"},"or")," Bad requests | Server cannot or will not process the request"))),(0,l.kt)("div",null))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("div",{style:{display:"flex"}},(0,l.kt)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"}},(0,l.kt)("code",null,"401")),(0,l.kt)("div",null,(0,l.kt)("p",null,"Unauthorized | Incorrect X-API-Key or X-API-Key not have access to this model"))),(0,l.kt)("div",null))),(0,l.kt)("tr",null,(0,l.kt)("td",null,(0,l.kt)("div",{style:{display:"flex"}},(0,l.kt)("div",{style:{marginRight:"var(--ifm-table-cell-padding)"}},(0,l.kt)("code",null,"415")),(0,l.kt)("div",null,(0,l.kt)("p",null,"Can't decode ","[filename]"," | Unsupported file format"))),(0,l.kt)("div",null))))))}d.isMDXComponent=!0}}]);